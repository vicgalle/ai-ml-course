{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, **very few people train an entire Convolutional Network from scratch**, because it is relatively rare to have a dataset of sufficient size. Instead, it is common to **use a pretrained a CNN on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories)**, and then use the CNN either as an **initialization or a fixed feature extractor** for the task of interest to us.\n",
    "\n",
    "![images](images/tf1.png)\n",
    "\n",
    "* **Pretrained models**. Since modern CNNs take 2-3 weeks to train across multiple GPUs on ImageNet, it is common to see people release their final parameters for the benefit of others. In the previous notebook we already saw how to use a pretrained model from PyTorch's `torchvision` module.\n",
    "\n",
    "There are two main ways to do transfer learning with a pre-trained model:\n",
    "\n",
    "* **Feature Extraction**.  Take a CNN pretrained on ImageNet, remove the last linear layers, then treat the remaining of the CNN as a feature extractor for the new dataset. In an AlexNet for example, this would compute a 4096-D vector for every image. We call these features CNN codes. Once you extract the 4096-D codes for all images, train a standard classifier (i.e. Logistic Regression or Random Forest).\n",
    "\n",
    "* **Fine-Tuning**. The second strategy is to not only replace and retrain the classifier on top of the CNN on the new dataset, but to also fine-tune the weights of the pretrained network by continuing the training on our new dataset. It is possible to fine-tune all the layers of the CNN, or it's possible to keep some of the earlier layers fixed (due to overfitting concerns) and only fine-tune the later layers.\n",
    "\n",
    "In the case of the VGG architecture, this is a diagram of what the network looks like:\n",
    "\n",
    "![images](images/tf2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When and how to do transfer learning?\n",
    "\n",
    "* **How much data do you have?**. If you have a small dataset, it is not a good idea to fine-tune a full CNN. Instead, you should use feature extraction, training a linear classifier on top of the CNN codes.\n",
    "\n",
    "* **How similar is the data to the pretrained data?**. If your new dataset is very different from the dataset on which the original model was trained, it may be better to train a new CNN from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why to do transfer learning?\n",
    "\n",
    "The main advantage of transfer learning is the ability to empower you to achieve better accuracy on your tasks. We can break down its advantages as follows:\n",
    "\n",
    "* **Training efficiency**: When you start with a pre-trained model that has already learned the general features of the data, you then only need to fine-tune the model to your specific task, which can be done much more quickly (i.e. using fewer training epochs).\n",
    "* **Model accuracy**: Using transfer learning can give you a significant performance boost compared to training a model from scratch using the same amount of resources. Choosing the right pre-trained model for transfer-learning for your specific task is important though.\n",
    "* **Training data size**: Since a pre-trained model would have already learned to identify many of the features that overlap with your task-specific features, you can train the pre-trained model with less domain-specific data. This is useful if you don’t have as much labeled data for your specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning with Pytorch: extracting features from a CNN\n",
    "\n",
    "The process is straightforward. We first start with a pre-trained CNN, for example EfficientNet-B4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victorgallego/miniforge3/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import efficientnet_b4, EfficientNet_B4_Weights\n",
    "\n",
    "# Step 1: Initialize model with the best available weights\n",
    "weights = EfficientNet_B4_Weights.DEFAULT\n",
    "model = efficientnet_b4(weights=weights)\n",
    "model.eval()  # disables gradient calculation for inference.\n",
    "\n",
    "# Step 2: Initialize the preprocess function\n",
    "preprocess = weights.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to output the names of the layers with the folowing function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['x',\n",
       "  'features.0',\n",
       "  'features.1.0.block.0',\n",
       "  'features.1.0.block.1',\n",
       "  'features.1.0.block.2',\n",
       "  'features.1.1.block.0',\n",
       "  'features.1.1.block.1',\n",
       "  'features.1.1.block.2',\n",
       "  'features.1.1.stochastic_depth',\n",
       "  'features.1.1.add',\n",
       "  'features.2.0.block.0',\n",
       "  'features.2.0.block.1',\n",
       "  'features.2.0.block.2',\n",
       "  'features.2.0.block.3',\n",
       "  'features.2.1.block.0',\n",
       "  'features.2.1.block.1',\n",
       "  'features.2.1.block.2',\n",
       "  'features.2.1.block.3',\n",
       "  'features.2.1.stochastic_depth',\n",
       "  'features.2.1.add',\n",
       "  'features.2.2.block.0',\n",
       "  'features.2.2.block.1',\n",
       "  'features.2.2.block.2',\n",
       "  'features.2.2.block.3',\n",
       "  'features.2.2.stochastic_depth',\n",
       "  'features.2.2.add',\n",
       "  'features.2.3.block.0',\n",
       "  'features.2.3.block.1',\n",
       "  'features.2.3.block.2',\n",
       "  'features.2.3.block.3',\n",
       "  'features.2.3.stochastic_depth',\n",
       "  'features.2.3.add',\n",
       "  'features.3.0.block.0',\n",
       "  'features.3.0.block.1',\n",
       "  'features.3.0.block.2',\n",
       "  'features.3.0.block.3',\n",
       "  'features.3.1.block.0',\n",
       "  'features.3.1.block.1',\n",
       "  'features.3.1.block.2',\n",
       "  'features.3.1.block.3',\n",
       "  'features.3.1.stochastic_depth',\n",
       "  'features.3.1.add',\n",
       "  'features.3.2.block.0',\n",
       "  'features.3.2.block.1',\n",
       "  'features.3.2.block.2',\n",
       "  'features.3.2.block.3',\n",
       "  'features.3.2.stochastic_depth',\n",
       "  'features.3.2.add',\n",
       "  'features.3.3.block.0',\n",
       "  'features.3.3.block.1',\n",
       "  'features.3.3.block.2',\n",
       "  'features.3.3.block.3',\n",
       "  'features.3.3.stochastic_depth',\n",
       "  'features.3.3.add',\n",
       "  'features.4.0.block.0',\n",
       "  'features.4.0.block.1',\n",
       "  'features.4.0.block.2',\n",
       "  'features.4.0.block.3',\n",
       "  'features.4.1.block.0',\n",
       "  'features.4.1.block.1',\n",
       "  'features.4.1.block.2',\n",
       "  'features.4.1.block.3',\n",
       "  'features.4.1.stochastic_depth',\n",
       "  'features.4.1.add',\n",
       "  'features.4.2.block.0',\n",
       "  'features.4.2.block.1',\n",
       "  'features.4.2.block.2',\n",
       "  'features.4.2.block.3',\n",
       "  'features.4.2.stochastic_depth',\n",
       "  'features.4.2.add',\n",
       "  'features.4.3.block.0',\n",
       "  'features.4.3.block.1',\n",
       "  'features.4.3.block.2',\n",
       "  'features.4.3.block.3',\n",
       "  'features.4.3.stochastic_depth',\n",
       "  'features.4.3.add',\n",
       "  'features.4.4.block.0',\n",
       "  'features.4.4.block.1',\n",
       "  'features.4.4.block.2',\n",
       "  'features.4.4.block.3',\n",
       "  'features.4.4.stochastic_depth',\n",
       "  'features.4.4.add',\n",
       "  'features.4.5.block.0',\n",
       "  'features.4.5.block.1',\n",
       "  'features.4.5.block.2',\n",
       "  'features.4.5.block.3',\n",
       "  'features.4.5.stochastic_depth',\n",
       "  'features.4.5.add',\n",
       "  'features.5.0.block.0',\n",
       "  'features.5.0.block.1',\n",
       "  'features.5.0.block.2',\n",
       "  'features.5.0.block.3',\n",
       "  'features.5.1.block.0',\n",
       "  'features.5.1.block.1',\n",
       "  'features.5.1.block.2',\n",
       "  'features.5.1.block.3',\n",
       "  'features.5.1.stochastic_depth',\n",
       "  'features.5.1.add',\n",
       "  'features.5.2.block.0',\n",
       "  'features.5.2.block.1',\n",
       "  'features.5.2.block.2',\n",
       "  'features.5.2.block.3',\n",
       "  'features.5.2.stochastic_depth',\n",
       "  'features.5.2.add',\n",
       "  'features.5.3.block.0',\n",
       "  'features.5.3.block.1',\n",
       "  'features.5.3.block.2',\n",
       "  'features.5.3.block.3',\n",
       "  'features.5.3.stochastic_depth',\n",
       "  'features.5.3.add',\n",
       "  'features.5.4.block.0',\n",
       "  'features.5.4.block.1',\n",
       "  'features.5.4.block.2',\n",
       "  'features.5.4.block.3',\n",
       "  'features.5.4.stochastic_depth',\n",
       "  'features.5.4.add',\n",
       "  'features.5.5.block.0',\n",
       "  'features.5.5.block.1',\n",
       "  'features.5.5.block.2',\n",
       "  'features.5.5.block.3',\n",
       "  'features.5.5.stochastic_depth',\n",
       "  'features.5.5.add',\n",
       "  'features.6.0.block.0',\n",
       "  'features.6.0.block.1',\n",
       "  'features.6.0.block.2',\n",
       "  'features.6.0.block.3',\n",
       "  'features.6.1.block.0',\n",
       "  'features.6.1.block.1',\n",
       "  'features.6.1.block.2',\n",
       "  'features.6.1.block.3',\n",
       "  'features.6.1.stochastic_depth',\n",
       "  'features.6.1.add',\n",
       "  'features.6.2.block.0',\n",
       "  'features.6.2.block.1',\n",
       "  'features.6.2.block.2',\n",
       "  'features.6.2.block.3',\n",
       "  'features.6.2.stochastic_depth',\n",
       "  'features.6.2.add',\n",
       "  'features.6.3.block.0',\n",
       "  'features.6.3.block.1',\n",
       "  'features.6.3.block.2',\n",
       "  'features.6.3.block.3',\n",
       "  'features.6.3.stochastic_depth',\n",
       "  'features.6.3.add',\n",
       "  'features.6.4.block.0',\n",
       "  'features.6.4.block.1',\n",
       "  'features.6.4.block.2',\n",
       "  'features.6.4.block.3',\n",
       "  'features.6.4.stochastic_depth',\n",
       "  'features.6.4.add',\n",
       "  'features.6.5.block.0',\n",
       "  'features.6.5.block.1',\n",
       "  'features.6.5.block.2',\n",
       "  'features.6.5.block.3',\n",
       "  'features.6.5.stochastic_depth',\n",
       "  'features.6.5.add',\n",
       "  'features.6.6.block.0',\n",
       "  'features.6.6.block.1',\n",
       "  'features.6.6.block.2',\n",
       "  'features.6.6.block.3',\n",
       "  'features.6.6.stochastic_depth',\n",
       "  'features.6.6.add',\n",
       "  'features.6.7.block.0',\n",
       "  'features.6.7.block.1',\n",
       "  'features.6.7.block.2',\n",
       "  'features.6.7.block.3',\n",
       "  'features.6.7.stochastic_depth',\n",
       "  'features.6.7.add',\n",
       "  'features.7.0.block.0',\n",
       "  'features.7.0.block.1',\n",
       "  'features.7.0.block.2',\n",
       "  'features.7.0.block.3',\n",
       "  'features.7.1.block.0',\n",
       "  'features.7.1.block.1',\n",
       "  'features.7.1.block.2',\n",
       "  'features.7.1.block.3',\n",
       "  'features.7.1.stochastic_depth',\n",
       "  'features.7.1.add',\n",
       "  'features.8',\n",
       "  'avgpool',\n",
       "  'flatten',\n",
       "  'classifier.0',\n",
       "  'classifier.1'],\n",
       " ['x',\n",
       "  'features.0',\n",
       "  'features.1.0.block.0',\n",
       "  'features.1.0.block.1',\n",
       "  'features.1.0.block.2',\n",
       "  'features.1.1.block.0',\n",
       "  'features.1.1.block.1',\n",
       "  'features.1.1.block.2',\n",
       "  'features.1.1.stochastic_depth',\n",
       "  'features.1.1.add',\n",
       "  'features.2.0.block.0',\n",
       "  'features.2.0.block.1',\n",
       "  'features.2.0.block.2',\n",
       "  'features.2.0.block.3',\n",
       "  'features.2.1.block.0',\n",
       "  'features.2.1.block.1',\n",
       "  'features.2.1.block.2',\n",
       "  'features.2.1.block.3',\n",
       "  'features.2.1.stochastic_depth',\n",
       "  'features.2.1.add',\n",
       "  'features.2.2.block.0',\n",
       "  'features.2.2.block.1',\n",
       "  'features.2.2.block.2',\n",
       "  'features.2.2.block.3',\n",
       "  'features.2.2.stochastic_depth',\n",
       "  'features.2.2.add',\n",
       "  'features.2.3.block.0',\n",
       "  'features.2.3.block.1',\n",
       "  'features.2.3.block.2',\n",
       "  'features.2.3.block.3',\n",
       "  'features.2.3.stochastic_depth',\n",
       "  'features.2.3.add',\n",
       "  'features.3.0.block.0',\n",
       "  'features.3.0.block.1',\n",
       "  'features.3.0.block.2',\n",
       "  'features.3.0.block.3',\n",
       "  'features.3.1.block.0',\n",
       "  'features.3.1.block.1',\n",
       "  'features.3.1.block.2',\n",
       "  'features.3.1.block.3',\n",
       "  'features.3.1.stochastic_depth',\n",
       "  'features.3.1.add',\n",
       "  'features.3.2.block.0',\n",
       "  'features.3.2.block.1',\n",
       "  'features.3.2.block.2',\n",
       "  'features.3.2.block.3',\n",
       "  'features.3.2.stochastic_depth',\n",
       "  'features.3.2.add',\n",
       "  'features.3.3.block.0',\n",
       "  'features.3.3.block.1',\n",
       "  'features.3.3.block.2',\n",
       "  'features.3.3.block.3',\n",
       "  'features.3.3.stochastic_depth',\n",
       "  'features.3.3.add',\n",
       "  'features.4.0.block.0',\n",
       "  'features.4.0.block.1',\n",
       "  'features.4.0.block.2',\n",
       "  'features.4.0.block.3',\n",
       "  'features.4.1.block.0',\n",
       "  'features.4.1.block.1',\n",
       "  'features.4.1.block.2',\n",
       "  'features.4.1.block.3',\n",
       "  'features.4.1.stochastic_depth',\n",
       "  'features.4.1.add',\n",
       "  'features.4.2.block.0',\n",
       "  'features.4.2.block.1',\n",
       "  'features.4.2.block.2',\n",
       "  'features.4.2.block.3',\n",
       "  'features.4.2.stochastic_depth',\n",
       "  'features.4.2.add',\n",
       "  'features.4.3.block.0',\n",
       "  'features.4.3.block.1',\n",
       "  'features.4.3.block.2',\n",
       "  'features.4.3.block.3',\n",
       "  'features.4.3.stochastic_depth',\n",
       "  'features.4.3.add',\n",
       "  'features.4.4.block.0',\n",
       "  'features.4.4.block.1',\n",
       "  'features.4.4.block.2',\n",
       "  'features.4.4.block.3',\n",
       "  'features.4.4.stochastic_depth',\n",
       "  'features.4.4.add',\n",
       "  'features.4.5.block.0',\n",
       "  'features.4.5.block.1',\n",
       "  'features.4.5.block.2',\n",
       "  'features.4.5.block.3',\n",
       "  'features.4.5.stochastic_depth',\n",
       "  'features.4.5.add',\n",
       "  'features.5.0.block.0',\n",
       "  'features.5.0.block.1',\n",
       "  'features.5.0.block.2',\n",
       "  'features.5.0.block.3',\n",
       "  'features.5.1.block.0',\n",
       "  'features.5.1.block.1',\n",
       "  'features.5.1.block.2',\n",
       "  'features.5.1.block.3',\n",
       "  'features.5.1.stochastic_depth',\n",
       "  'features.5.1.add',\n",
       "  'features.5.2.block.0',\n",
       "  'features.5.2.block.1',\n",
       "  'features.5.2.block.2',\n",
       "  'features.5.2.block.3',\n",
       "  'features.5.2.stochastic_depth',\n",
       "  'features.5.2.add',\n",
       "  'features.5.3.block.0',\n",
       "  'features.5.3.block.1',\n",
       "  'features.5.3.block.2',\n",
       "  'features.5.3.block.3',\n",
       "  'features.5.3.stochastic_depth',\n",
       "  'features.5.3.add',\n",
       "  'features.5.4.block.0',\n",
       "  'features.5.4.block.1',\n",
       "  'features.5.4.block.2',\n",
       "  'features.5.4.block.3',\n",
       "  'features.5.4.stochastic_depth',\n",
       "  'features.5.4.add',\n",
       "  'features.5.5.block.0',\n",
       "  'features.5.5.block.1',\n",
       "  'features.5.5.block.2',\n",
       "  'features.5.5.block.3',\n",
       "  'features.5.5.stochastic_depth',\n",
       "  'features.5.5.add',\n",
       "  'features.6.0.block.0',\n",
       "  'features.6.0.block.1',\n",
       "  'features.6.0.block.2',\n",
       "  'features.6.0.block.3',\n",
       "  'features.6.1.block.0',\n",
       "  'features.6.1.block.1',\n",
       "  'features.6.1.block.2',\n",
       "  'features.6.1.block.3',\n",
       "  'features.6.1.stochastic_depth',\n",
       "  'features.6.1.add',\n",
       "  'features.6.2.block.0',\n",
       "  'features.6.2.block.1',\n",
       "  'features.6.2.block.2',\n",
       "  'features.6.2.block.3',\n",
       "  'features.6.2.stochastic_depth',\n",
       "  'features.6.2.add',\n",
       "  'features.6.3.block.0',\n",
       "  'features.6.3.block.1',\n",
       "  'features.6.3.block.2',\n",
       "  'features.6.3.block.3',\n",
       "  'features.6.3.stochastic_depth',\n",
       "  'features.6.3.add',\n",
       "  'features.6.4.block.0',\n",
       "  'features.6.4.block.1',\n",
       "  'features.6.4.block.2',\n",
       "  'features.6.4.block.3',\n",
       "  'features.6.4.stochastic_depth',\n",
       "  'features.6.4.add',\n",
       "  'features.6.5.block.0',\n",
       "  'features.6.5.block.1',\n",
       "  'features.6.5.block.2',\n",
       "  'features.6.5.block.3',\n",
       "  'features.6.5.stochastic_depth',\n",
       "  'features.6.5.add',\n",
       "  'features.6.6.block.0',\n",
       "  'features.6.6.block.1',\n",
       "  'features.6.6.block.2',\n",
       "  'features.6.6.block.3',\n",
       "  'features.6.6.stochastic_depth',\n",
       "  'features.6.6.add',\n",
       "  'features.6.7.block.0',\n",
       "  'features.6.7.block.1',\n",
       "  'features.6.7.block.2',\n",
       "  'features.6.7.block.3',\n",
       "  'features.6.7.stochastic_depth',\n",
       "  'features.6.7.add',\n",
       "  'features.7.0.block.0',\n",
       "  'features.7.0.block.1',\n",
       "  'features.7.0.block.2',\n",
       "  'features.7.0.block.3',\n",
       "  'features.7.1.block.0',\n",
       "  'features.7.1.block.1',\n",
       "  'features.7.1.block.2',\n",
       "  'features.7.1.block.3',\n",
       "  'features.7.1.stochastic_depth',\n",
       "  'features.7.1.add',\n",
       "  'features.8',\n",
       "  'avgpool',\n",
       "  'flatten',\n",
       "  'classifier.0',\n",
       "  'classifier.1'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_graph_node_names(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the last layers of the network are:\n",
    "\n",
    "```\n",
    "'flatten',\n",
    "'classifier.0',\n",
    "'classifier.1']\n",
    "```\n",
    "\n",
    "That is, after the convolutional layers, we have a flatten layer, and then two linear layers to perform the classification.\n",
    "\n",
    "* The classification layers are not interesting to use, because they are specialized for the ImageNet dataset (1000 clases). We will remove them and use the rest of the network as a feature extractor.\n",
    "* So we only use the output of the flatten layer as the features of our data.\n",
    "\n",
    "To do this, we will use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_before_final_classifiers = 'flatten'\n",
    "\n",
    "return_nodes = {\n",
    "    layer_before_final_classifiers: layer_before_final_classifiers\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = create_feature_extractor(model, return_nodes=return_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, `feature_extractor`, is just the convolutional layers of the network, and we can use it to extract features from our data, and compute a better representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples with some input images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 380, 380])\n"
     ]
    }
   ],
   "source": [
    "img = read_image('images/husky.png')\n",
    "\n",
    "image_processed = preprocess(img).unsqueeze(0)\n",
    "\n",
    "print(image_processed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the images are RGB, the input to the network is a tensor of shape `(1, 3, 380, 380)`. We need to add a singleton dimension to the input tensor, because the network expects a batch of images, and we only have one image. So this is just a way of adding a batch dimension to the input tensor.\n",
    "\n",
    "We can use the `feature_extractor` to extract the features of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_code = feature_extractor(image_processed)[layer_before_final_classifiers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1792])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_code.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice now the dimensions change:\n",
    "\n",
    "* 1 because we only have one image in our batch\n",
    "* 1792 because that is the number of remaining dimensions after the convolutional layers. \n",
    "\n",
    "So instead of representing our images with 380x380x3=433200 dimensions, we can represent them with 1792 dimensions, which is a much more compact representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also work with several images at the same time, in a single batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_processed = []\n",
    "\n",
    "for img_path in ['images/husky.png', 'images/bank.png']:\n",
    "    img = read_image(img_path)\n",
    "    img_processed = preprocess(img)\n",
    "    imgs_processed.append(img_processed)\n",
    "\n",
    "imgs_processed = torch.stack(imgs_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 380, 380])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_codes = feature_extractor(imgs_processed)[layer_before_final_classifiers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1792])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_codes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are working with a batch of 2 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Example: Human Presence Analysis in Stock Images\n",
    "\n",
    "Unlocking Sales Potential with **Human Features Presence Analysis**.\n",
    "\n",
    "The primary goal of this example is to develop a model to assist the sales/marketing and studio departments at a fashion firm (Parfois). This model will offer a clear overview of the presence of human models for different products on their website, ultimately allowing informed decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](images/parfois.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First, we will load a csv with the image paths and the corresponding labels (human presence or not).\n",
    "\n",
    "We have around 70 labeled images, which is a very small dataset. We will use transfer learning to solve this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data_parfois/images_labels.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_file_name</th>\n",
       "      <th>label</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>216394_BM_1y.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>216394_BM_2yf.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>216394_BM_3y.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>216394_BM_4y.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>216394_BM_5y.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     image_file_name  label  Unnamed: 2  Unnamed: 3  Unnamed: 4\n",
       "0   216394_BM_1y.jpg      1         NaN         NaN         NaN\n",
       "1  216394_BM_2yf.jpg      0         NaN         NaN         NaN\n",
       "2   216394_BM_3y.jpg      1         NaN         NaN         NaN\n",
       "3   216394_BM_4y.jpg      0         NaN         NaN         NaN\n",
       "4   216394_BM_5y.jpg      1         NaN         NaN         NaN"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: xlabel='label'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.value_counts('label').plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We use a pretrained CNN to extract the features from the images, one by one.\n",
    "\n",
    "Finally, we will store it in a single tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /Users/victorgallego/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
      "100%|██████████| 20.5M/20.5M [00:01<00:00, 17.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.io import read_image\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "\n",
    "# Step 1: Initialize model with the best available weights\n",
    "weights = EfficientNet_B0_Weights.DEFAULT\n",
    "model = efficientnet_b0(weights=weights)\n",
    "model.eval()  # disables gradient calculation for inference.\n",
    "\n",
    "# Step 2: Initialize the preprocess function\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "\n",
    "layer_before_final_classifiers = 'flatten'\n",
    "\n",
    "return_nodes = {\n",
    "    layer_before_final_classifiers: layer_before_final_classifiers\n",
    "}\n",
    "\n",
    "feature_extractor = create_feature_extractor(model, return_nodes=return_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 1/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 2/75\n",
      "Processing image 3/75\n",
      "Processing image 4/75\n",
      "Processing image 5/75\n",
      "Processing image 6/75\n",
      "Processing image 7/75\n",
      "Processing image 8/75\n",
      "Processing image 9/75\n",
      "Processing image 10/75\n",
      "Processing image 11/75\n",
      "Processing image 12/75\n",
      "Processing image 13/75\n",
      "Processing image 14/75\n",
      "Processing image 15/75\n",
      "Processing image 16/75\n",
      "Processing image 17/75\n",
      "Processing image 18/75\n",
      "Processing image 19/75\n",
      "Processing image 20/75\n",
      "Processing image 21/75\n",
      "Processing image 22/75\n",
      "Processing image 23/75\n",
      "Processing image 24/75\n",
      "Processing image 25/75\n",
      "Processing image 26/75\n",
      "Processing image 27/75\n",
      "Processing image 28/75\n",
      "Processing image 29/75\n",
      "Processing image 30/75\n",
      "Processing image 31/75\n",
      "Processing image 32/75\n",
      "Processing image 33/75\n",
      "Processing image 34/75\n",
      "Processing image 35/75\n",
      "Processing image 36/75\n",
      "Processing image 37/75\n",
      "Processing image 38/75\n",
      "Processing image 39/75\n",
      "Processing image 40/75\n",
      "Processing image 41/75\n",
      "Processing image 42/75\n",
      "Processing image 43/75\n",
      "Processing image 44/75\n",
      "Processing image 45/75\n",
      "Processing image 46/75\n",
      "Processing image 47/75\n",
      "Processing image 48/75\n",
      "Processing image 49/75\n",
      "Processing image 50/75\n",
      "Processing image 51/75\n",
      "Processing image 52/75\n",
      "Processing image 53/75\n",
      "Processing image 54/75\n",
      "Processing image 55/75\n",
      "Processing image 56/75\n",
      "Processing image 57/75\n",
      "Processing image 58/75\n",
      "Processing image 59/75\n",
      "Processing image 60/75\n",
      "Processing image 61/75\n",
      "Processing image 62/75\n",
      "Processing image 63/75\n",
      "Processing image 64/75\n",
      "Processing image 65/75\n",
      "Processing image 66/75\n",
      "Processing image 67/75\n",
      "Processing image 68/75\n",
      "Processing image 69/75\n",
      "Processing image 70/75\n",
      "Processing image 71/75\n",
      "Processing image 72/75\n",
      "Processing image 73/75\n",
      "Processing image 74/75\n",
      "Processing image 75/75\n"
     ]
    }
   ],
   "source": [
    "image_names = df['image_file_name'].values\n",
    "\n",
    "cnn_codes = []\n",
    "for i, image_name in enumerate(image_names):\n",
    "    print(f'Processing image {i+1}/{len(image_names)}')\n",
    "    with torch.no_grad():\n",
    "        img = read_image(f'data_parfois/{image_name}')\n",
    "        img_processed = preprocess(img).unsqueeze(0)\n",
    "        cnn_code = feature_extractor(img_processed)[layer_before_final_classifiers]\n",
    "    cnn_codes.append(cnn_code)\n",
    "\n",
    "\n",
    "cnn_codes = torch.cat(cnn_codes)  # stack all the processed tensors into a single tensor    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 1280)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_codes_numpy = cnn_codes.numpy()\n",
    "cnn_codes_numpy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = df.label.values\n",
    "\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now, we can continue with sklearn, to make a split of the data and train a classifier for our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cnn_codes_numpy, labels, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        18\n",
      "           1       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        23\n",
      "   macro avg       1.00      1.00      1.00        23\n",
      "weighted avg       1.00      1.00      1.00        23\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97        18\n",
      "           1       1.00      0.80      0.89         5\n",
      "\n",
      "    accuracy                           0.96        23\n",
      "   macro avg       0.97      0.90      0.93        23\n",
      "weighted avg       0.96      0.96      0.95        23\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** What happens if we use a smaller CNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take-aways ⚠️\n",
    "\n",
    "* **Transfer learning** is a powerful technique that can be used to build accurate models with a small amount of data.\n",
    "\n",
    "* In our Parfois example, we only had 75 labelled images, but we were able to build a model that could predict the presence of humans in images with perfect degree of accuracy.\n",
    "\n",
    "  * Now, we could use our trained model to classify all the images in the website/marketplace, and then use this information to make decisions about the images that need to be replaced or improved:\n",
    "  \n",
    "  1. For example, we can identify the images that do not have human presence and replace them with images that do have human presence.\n",
    "  2. Or we could compute the Click-Through Rate (CTR) for the images with human presence and without human presence, and then use this information to see which group of images is more effective in driving sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}